# Architecture Decision Records (ADRs)

## ADR-001: Monorepo with RPC-Only Architecture

**Decision:** Use single Workers repository handling frontend and backend, with all internal communication via RPC service bindings (no exposed HTTP APIs).

**Rationale:**
- Simplifies deployment (single wrangler.toml)
- Reduces network overhead (RPC faster than HTTP)
- Improves security (no exposed internal endpoints)
- Easier to maintain type safety across boundaries

**Affects:** All epics

---

## ADR-002: Single TestAgent Durable Object Per Test

**Decision:** Each test run is managed by one TestAgent Durable Object instance (DO ID = test UUID), persisting across all 4 phases and retries.

**Rationale:**
- Simplifies state management (no coordination between multiple agents)
- Browser session persists across phases (faster, maintains game state)
- Evidence accumulates naturally in DO state
- Clear ownership: TestAgent owns entire test execution
- Workflow focuses on orchestration and retry logic

**Alternative Considered:** Multiple specialized agents (LoadAgent, DiscoveryAgent, etc.)
- Rejected due to complexity of state coordination and browser session handoff

**Affects:** Epic 1 (infrastructure), Epic 2 (TestAgent implementation)

---

## ADR-003: Workflow Auto-Retry with TestAgent Error Awareness

**Decision:** Cloudflare Workflows handle automatic retry with exponential backoff, but TestAgent receives error context to try adaptive strategies on retry attempts.

**Rationale:**
- Leverages Workflows' built-in retry mechanism (no custom implementation)
- TestAgent can adapt behavior based on previous failure
- Graceful degradation: continue with partial data if retries exhausted
- User-friendly error messages: all technical errors translated

**Example:** If control discovery fails, TestAgent tries alternative strategy (use inputSchema) on retry.

**Affects:** Epic 1 (Workflow setup), Epic 2 (TestAgent error handling)

---

## ADR-004: AI Gateway as Primary Entry Point for All AI Requests

**Decision:** All AI requests (TestAgent decisions, evaluation) route through Cloudflare AI Gateway, not directly to AI providers.

**Rationale:**
- Unified observability: single place to monitor AI usage
- Cost tracking: automatic tracking per test run
- Automatic failover: if Workers AI fails, fallback to OpenAI/Anthropic
- Caching: 15-minute TTL reduces costs for repeated evaluations
- Future-proof: easy to swap AI providers without code changes

**Affects:** Epic 1 (AI Gateway setup), Epic 2 (TestAgent AI calls)

---

## ADR-005: Direct Wrangler Deploy (No CI/CD Pipeline)

**Decision:** Deploy directly to production/staging using `wrangler deploy`, skipping traditional CI/CD pipeline.

**Rationale:**
- Wrangler handles build, bundle, and deploy in one command
- Cloudflare edge deployment is instant (< 30 seconds)
- Staging and production environments via `--env` flag
- Rollback available via `wrangler rollback`
- Eliminates CI/CD complexity and maintenance overhead

**Trade-off:** No automated tests before deploy (acceptable for rapid prototyping, add later if needed)

**Affects:** All epics (development workflow)

---

## ADR-006: WebSocket for Real-Time Updates, Polling as Fallback

**Decision:** Use Agents SDK WebSocket API for real-time progress updates, with 3-second polling as fallback if WebSocket unavailable.

**Rationale:**
- WebSocket provides instant updates (better UX)
- Built into Agents SDK (no additional infrastructure)
- Polling fallback ensures functionality if WebSocket fails
- Rate limit WebSocket (1 event/5 sec) to avoid spam

**Alternative Considered:** Polling-only approach
- Rejected due to poor UX (3-second delay on every update)

**Affects:** Epic 2 (TestAgent events), Epic 3 (Dashboard UI)

---

## ADR-007: Agent SQL for Ephemeral Per-Test Data, D1 for Cross-Test Metadata

**Decision:** Use Agent SQL (built into Durable Objects) for per-test reasoning/decisions, and D1 for persistent cross-test metadata.

**Rationale:**
- Agent SQL perfect for ephemeral data (AI decisions, action history)
- D1 optimized for cross-test queries (historical trends, test list)
- Clear separation of concerns: per-test vs. cross-test data
- No need to clean up Agent SQL (DO destroyed after test complete)

**Affects:** Epic 1 (database setup), Epic 2 (TestAgent storage), Epic 3 (Dashboard queries)

---

_Generated by BMAD Decision Architecture Workflow v1.3.2_
_Date: November 4, 2025_
_For: Adam_

